# Há»‡ thá»‘ng ETL Thá»i gian thá»±c vá»›i Northwind Database

![Banner](https://via.placeholder.com/800x200.png?text=ETL+Thá»i+gian+thá»±c+vá»›i+Northwind+Database)  
*HÃ¬nh áº£nh minh há»a há»‡ thá»‘ng (thay tháº¿ báº±ng hÃ¬nh áº£nh thá»±c táº¿ náº¿u cÃ³)*

## ğŸš€ Má»¥c tiÃªu dá»± Ã¡n

XÃ¢y dá»±ng há»‡ thá»‘ng **ETL (Extract, Transform, Load) thá»i gian thá»±c** sá»­ dá»¥ng **Northwind sample database** (MySQL) Ä‘á»ƒ:
- **Extract**: Báº¯t cÃ¡c thay Ä‘á»•i (INSERT, UPDATE, DELETE) tá»« cÃ¡c báº£ng `customers`, `orders`, `order_details` báº±ng **Debezium CDC**.
- **Transform**: Xá»­ lÃ½, lÃ m sáº¡ch, vÃ  tÃ­nh toÃ¡n aggregation (tá»•ng doanh thu theo ngÃ y, sá»‘ Ä‘Æ¡n hÃ ng theo khÃ¡ch hÃ ng) báº±ng **Spark Structured Streaming**.
- **Load**: LÆ°u dá»¯ liá»‡u Ä‘Ã£ xá»­ lÃ½ vÃ o **HDFS** dÆ°á»›i dáº¡ng Parquet.

**YÃªu cáº§u chá»©c nÄƒng**:
- Theo dÃµi thay Ä‘á»•i trÃªn báº£ng `customers`, `orders`, `order_details`.
- LÃ m sáº¡ch dá»¯ liá»‡u (loáº¡i bá» null, xá»­ lÃ½ duplicate).
- TÃ­nh toÃ¡n:
  - Tá»•ng doanh thu (`order_details.quantity * order_details.unit_price`) theo ngÃ y.
  - Sá»‘ Ä‘Æ¡n hÃ ng theo khÃ¡ch hÃ ng.
- LÆ°u káº¿t quáº£ vÃ o HDFS vá»›i cÃ¡c partition theo ngÃ y.
- TÃ¹y chá»n: Dashboard Grafana hiá»ƒn thá»‹ tá»•ng doanh thu theo thá»i gian thá»±c.

**Thá»i gian**: 4 tuáº§n (30/09/2025 - 28/10/2025)  
**NhÃ³m**: 5 thÃ nh viÃªn

---

## ğŸ“‘ Má»¥c lá»¥c

- [ğŸ“‹ MÃ´ táº£ há»‡ thá»‘ng](#-mÃ´-táº£-há»‡-thá»‘ng)
- [ğŸ”¹ Luá»“ng dá»¯ liá»‡u](#-luá»“ng-dá»¯ liá»‡u)
- [ğŸ“‹ Cáº¥u trÃºc dá»± Ã¡n](#-cáº¥u-trÃºc-dá»±-Ã¡n)
- [ğŸ”¹ YÃªu cáº§u](#-yÃªu-cáº§u)
- [ğŸ“¥ CÃ i Ä‘áº·t vÃ  cháº¡y demo](#-cÃ i-Ä‘áº·t-vÃ -cháº¡y-demo)
  - [CÃ i Ä‘áº·t mÃ´i trÆ°á»ng](#cÃ i-Ä‘áº·t-mÃ´i-trÆ°á»ng)
  - [Khá»Ÿi táº¡o dá»¯ liá»‡u Northwind](#khá»Ÿi-táº¡o-dá»¯-liá»‡u-northwind)
  - [ÄÄƒng kÃ½ Debezium Connector](#Ä‘Äƒng-kÃ½-debezium-connector)
  - [Cháº¡y Spark Streaming](#cháº¡y-spark-streaming)
  - [Kiá»ƒm tra káº¿t quáº£](#kiá»ƒm-tra-káº¿t-quáº£)
  - [Clean há»‡ thá»‘ng](#clean-há»‡-thá»‘ng)
- [ğŸ”¹ PhÃ¢n cÃ´ng cÃ´ng viá»‡c](#-phÃ¢n-cÃ´ng-cÃ´ng-viá»‡c)
- [ğŸ”¹ Lá»‹ch trÃ¬nh dá»± Ã¡n](#-lá»‹ch-trÃ¬nh-dá»±-Ã¡n)
- [ğŸ”¹ LÆ°u Ã½](#-lÆ°u-Ã½)
- [ğŸ”¹ Kháº¯c phá»¥c sá»± cá»‘](#-kháº¯c-phá»¥c-sá»±-cá»‘)
- [ğŸ”¹ CÃ´ng nghá»‡ sá»­ dá»¥ng](#-cÃ´ng-nghá»‡-sá»­-dá»¥ng)
- [ğŸ“§ LiÃªn há»‡](#-liÃªn-há»‡)

---

## ğŸ“‹ MÃ´ táº£ há»‡ thá»‘ng

Há»‡ thá»‘ng sá»­ dá»¥ng **Northwind database** (MySQL) lÃ m nguá»“n dá»¯ liá»‡u, vá»›i cÃ¡c thÃ nh pháº§n chÃ­nh:

- **MySQL**: LÆ°u trá»¯ Northwind database (`customers`, `orders`, `order_details`).
- **Debezium**: Theo dÃµi thay Ä‘á»•i (CDC) tá»« MySQL binlog, gá»­i Ä‘áº¿n Kafka topics (`cdc.northwind.customers`, `cdc.northwind.orders`, `cdc.northwind.order_details`).
- **Kafka**: HÃ ng Ä‘á»£i tin nháº¯n Ä‘á»ƒ truyá»n dá»¯ liá»‡u thá»i gian thá»±c.
- **Spark Structured Streaming**: Xá»­ lÃ½, lÃ m sáº¡ch, vÃ  tÃ­nh toÃ¡n aggregation tá»« Kafka.
- **HDFS**: Kho dá»¯ liá»‡u lÆ°u trá»¯ káº¿t quáº£ dÆ°á»›i dáº¡ng Parquet, partition theo ngÃ y.

![System Architecture](https://via.placeholder.com/600x300.png?text=Kiáº¿n+trÃºc+há»‡+thá»‘ng)  
*HÃ¬nh áº£nh kiáº¿n trÃºc há»‡ thá»‘ng (thay tháº¿ báº±ng hÃ¬nh áº£nh thá»±c táº¿ náº¿u cÃ³)*

---

## ğŸ”¹ Luá»“ng dá»¯ liá»‡u

1. **Extract**: Debezium báº¯t thay Ä‘á»•i (INSERT, UPDATE, DELETE) tá»« MySQL, gá»­i JSON messages Ä‘áº¿n Kafka topics.
2. **Transform**: Spark Structured Streaming Ä‘á»c tá»« Kafka, lÃ m sáº¡ch dá»¯ liá»‡u (loáº¡i bá» null, duplicate), join `orders` vÃ  `order_details`, tÃ­nh tá»•ng doanh thu vÃ  sá»‘ Ä‘Æ¡n hÃ ng.
3. **Load**: LÆ°u dá»¯ liá»‡u thÃ´ vÃ  aggregation vÃ o HDFS (`/northwind/raw/` vÃ  `/northwind/aggregated/`), partition theo ngÃ y (`year=2025/month=09/day=30`).

---

## ğŸ“‹ Cáº¥u trÃºc dá»± Ã¡n

```
etl-realtime-northwind/
â”œâ”€â”€ docker-compose.yml      # Cáº¥u hÃ¬nh Docker
â”œâ”€â”€ debezium_config.json    # Cáº¥u hÃ¬nh Debezium connector
â”œâ”€â”€ init_northwind.sql      # Script khá»Ÿi táº¡o Northwind database
â”œâ”€â”€ spark_job.py            # Spark Structured Streaming job
â”œâ”€â”€ assets/                 # HÃ¬nh áº£nh minh há»a
â””â”€â”€ README.md               # TÃ i liá»‡u hÆ°á»›ng dáº«n
```

---

## ğŸ”¹ YÃªu cáº§u

- **Git**: Clone dá»± Ã¡n.
- **Docker & Docker Compose**: Cháº¡y MySQL, Kafka, Spark, HDFS.
- **MongoDB Shell (`mongosh`)** hoáº·c **MongoDB Compass**: Kiá»ƒm tra dá»¯ liá»‡u (tÃ¹y chá»n cho debug).
- **HDFS**: Cluster 3 nodes hoáº·c local setup.
- **Python**: 3.9+ (pyspark, kafka-python).

---

## ğŸ“¥ CÃ i Ä‘áº·t vÃ  cháº¡y demo

### CÃ i Ä‘áº·t mÃ´i trÆ°á»ng

1. **Clone dá»± Ã¡n**:

   ```bash
   git clone https://github.com/your_username/etl-realtime-northwind.git
   cd etl-realtime-northwind
   ```

   ![Clone Project](https://via.placeholder.com/600x200.png?text=Clone+Project)  
   *HÃ¬nh áº£nh minh há»a clone dá»± Ã¡n*

2. **Táº¡o file `docker-compose.yml`**:

   ```yaml
   version: '3.8'
   services:
     mysql:
       image: mysql:8.0
       environment:
         MYSQL_ROOT_PASSWORD: root
         MYSQL_DATABASE: northwind
       ports:
         - "3306:3306"
       volumes:
         - ./init_northwind.sql:/docker-entrypoint-initdb.d/init_northwind.sql
       networks:
         - etl-network
     zookeeper:
       image: confluentinc/cp-zookeeper:7.4.0
       environment:
         ZOOKEEPER_CLIENT_PORT: 2181
         ZOOKEEPER_TICK_TIME: 2000
       networks:
         - etl-network
     kafka:
       image: confluentinc/cp-kafka:7.4.0
       depends_on:
         - zookeeper
       environment:
         KAFKA_BROKER_ID: 1
         KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
         KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:9093
         KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
         KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
         KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
       ports:
         - "9093:9093"
       networks:
         - etl-network
     connect:
       image: debezium/connect:2.3
       depends_on:
         - kafka
         - mysql
       environment:
         BOOTSTRAP_SERVERS: kafka:9092
         GROUP_ID: connect-group
         CONFIG_STORAGE_TOPIC: connect_configs
         OFFSET_STORAGE_TOPIC: connect_offsets
         STATUS_STORAGE_TOPIC: connect_status
       ports:
         - "8083:8083"
       networks:
         - etl-network
     spark-master:
       image: bitnami/spark:3.4.0
       environment:
         SPARK_MODE: master
       ports:
         - "7077:7077"
         - "8080:8080"
       networks:
         - etl-network
     spark-worker:
       image: bitnami/spark:3.4.0
       depends_on:
         - spark-master
       environment:
         SPARK_MODE: worker
         SPARK_MASTER_URL: spark://spark-master:7077
       networks:
         - etl-network
     hdfs-namenode:
       image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
       environment:
         CLUSTER_NAME: test
       ports:
         - "9870:9870"
       volumes:
         - hdfs-namenode-data:/hadoop/dfs/name
       networks:
         - etl-network
     hdfs-datanode:
       image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
       depends_on:
         - hdfs-namenode
       environment:
         SERVICE_PRECONDITION: hdfs-namenode:9870
       volumes:
         - hdfs-datanode-data:/hadoop/dfs/data
       networks:
         - etl-network
   volumes:
     hdfs-namenode-data:
     hdfs-datanode-data:
   networks:
     etl-network:
       driver: bridge
   ```

3. **Khá»Ÿi Ä‘á»™ng cÃ¡c container**:

   ```bash
   docker-compose up -d
   ```

   > **LÆ°u Ã½**: Kiá»ƒm tra container cháº¡y:
   > ```bash
   > docker ps
   > ```

4. **CÃ i Ä‘áº·t thÆ° viá»‡n Python trong container `spark-master`**:

   ```bash
   docker exec -it spark-master bash
   pip install pyspark kafka-python
   exit
   ```

### Khá»Ÿi táº¡o dá»¯ liá»‡u Northwind

5. **Táº¡o file `init_northwind.sql`**:

   ```sql
   CREATE DATABASE IF NOT EXISTS northwind;
   USE northwind;

   CREATE TABLE customers (
       customer_id VARCHAR(5) PRIMARY KEY,
       company_name VARCHAR(40),
       city VARCHAR(15)
   );

   CREATE TABLE orders (
       order_id INT PRIMARY KEY,
       customer_id VARCHAR(5),
       order_date DATE,
       FOREIGN KEY (customer_id) REFERENCES customers(customer_id)
   );

   CREATE TABLE order_details (
       order_id INT,
       product_id INT,
       unit_price DECIMAL(10,2),
       quantity INT,
       PRIMARY KEY (order_id, product_id),
       FOREIGN KEY (order_id) REFERENCES orders(order_id)
   );

   INSERT INTO customers (customer_id, company_name, city) VALUES
   ('ALFKI', 'Alfreds Futterkiste', 'Berlin'),
   ('ANATR', 'Ana Trujillo Emparedados', 'MÃ©xico D.F.'),
   ('ANTON', 'Antonio Moreno TaquerÃ­a', 'MÃ©xico D.F.');

   INSERT INTO orders (order_id, customer_id, order_date) VALUES
   (11077, 'ALFKI', '2025-09-30'),
   (11078, 'ANATR', '2025-09-30'),
   (11079, 'ANTON', '2025-09-30');

   INSERT INTO order_details (order_id, product_id, unit_price, quantity) VALUES
   (11077, 1, 18.00, 10),
   (11078, 2, 19.00, 5),
   (11079, 3, 10.00, 20);
   ```

6. **Ãp dá»¥ng schema Northwind**:

   - File `init_northwind.sql` sáº½ tá»± Ä‘á»™ng cháº¡y khi container `mysql` khá»Ÿi Ä‘á»™ng (do volume mapping trong `docker-compose.yml`).

7. **KÃ­ch hoáº¡t binlog cho MySQL**:

   ```bash
   docker exec -it mysql bash
   echo -e "[mysqld]\nlog-bin=mysql-bin\nbinlog-format=ROW\nserver-id=1" >> /etc/mysql/my.cnf
   exit
   docker-compose restart mysql
   ```

### ÄÄƒng kÃ½ Debezium Connector

8. **Táº¡o file `debezium_config.json`**:

   ```json
   {
     "name": "mysql-connector",
     "config": {
       "connector.class": "io.debezium.connector.mysql.MySqlConnector",
       "tasks.max": "1",
       "database.hostname": "mysql",
       "database.port": "3306",
       "database.user": "root",
       "database.password": "root",
       "database.server.id": "1",
       "database.server.name": "cdc",
       "database.include.list": "northwind",
       "table.include.list": "northwind.customers,northwind.orders,northwind.order_details",
       "database.history.kafka.bootstrap.servers": "kafka:9092",
       "database.history.kafka.topic": "cdc.northwind.schema-changes",
       "key.converter": "org.apache.kafka.connect.json.JsonConverter",
       "value.converter": "org.apache.kafka.connect.json.JsonConverter",
       "key.converter.schemas.enable": false,
       "value.converter.schemas.enable": false
     }
   }
   ```

9. **ÄÄƒng kÃ½ connector**:

   ```bash
   curl -X POST -H "Content-Type: application/json" --data @debezium_config.json http://localhost:8083/connectors
   ```

10. **Kiá»ƒm tra tráº¡ng thÃ¡i connector**:

    ```bash
    curl http://localhost:8083/connectors/mysql-connector/status
    ```

    > **Ká»³ vá»ng**: Tráº¡ng thÃ¡i `RUNNING`. Náº¿u lá»—i, kiá»ƒm tra log:
    > ```bash
    > docker logs connect
    > ```

### Cháº¡y Spark Streaming

11. **Táº¡o file `spark_job.py`**:

    ```python
    from pyspark.sql import SparkSession
    from pyspark.sql.functions import col, from_json, to_date, sum, count
    from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType

    # Khá»Ÿi táº¡o Spark session
    spark = SparkSession.builder \
        .appName("NorthwindETL") \
        .config("spark.hadoop.fs.defaultFS", "hdfs://hdfs-namenode:9870") \
        .getOrCreate()

    # Schema cho Kafka messages
    schema = StructType([
        StructField("payload", StructType([
            StructField("before", StringType(), True),
            StructField("after", StringType(), True),
            StructField("op", StringType(), True),
            StructField("ts_ms", StringType(), True)
        ]))
    ])

    # Äá»c tá»« Kafka
    df = spark.readStream \
        .format("kafka") \
        .option("kafka.bootstrap.servers", "kafka:9092") \
        .option("subscribe", "cdc.northwind.orders,cdc.northwind.order_details") \
        .option("startingOffsets", "earliest") \
        .load()

    # Parse JSON tá»« Kafka
    df = df.selectExpr("CAST(value AS STRING) as json") \
           .select(from_json(col("json"), schema).alias("data")) \
           .select("data.payload.after", "data.payload.op")

    # Parse chuá»—i JSON trong 'after'
    order_schema = StructType([
        StructField("order_id", IntegerType()),
        StructField("customer_id", StringType()),
        StructField("order_date", StringType())
    ])
    order_detail_schema = StructType([
        StructField("order_id", IntegerType()),
        StructField("product_id", IntegerType()),
        StructField("unit_price", DoubleType()),
        StructField("quantity", IntegerType())
    ])

    orders_df = df.where(col("op") == "c") \
                 .select(from_json(col("after"), order_schema).alias("order")) \
                 .select("order.*")
    order_details_df = df.where(col("op") == "c") \
                        .select(from_json(col("after"), order_detail_schema).alias("detail")) \
                        .select("detail.*")

    # Join vÃ  tÃ­nh toÃ¡n
    joined_df = orders_df.join(order_details_df, "order_id") \
                        .withColumn("revenue", col("unit_price") * col("quantity")) \
                        .withColumn("date", to_date(col("order_date")))

    # Aggregation: Tá»•ng doanh thu theo ngÃ y
    agg_df = joined_df.groupBy("date") \
                     .agg(sum("revenue").alias("total_revenue"),
                          count("order_id").alias("order_count"))

    # LÆ°u dá»¯ liá»‡u thÃ´ vÃ o HDFS
    raw_query = order_details_df.writeStream \
        .format("parquet") \
        .option("path", "hdfs://hdfs-namenode:9870/northwind/raw/order_details") \
        .option("checkpointLocation", "/tmp/spark-checkpoint/raw") \
        .partitionBy("order_id") \
        .start()

    # LÆ°u aggregation vÃ o HDFS
    agg_query = agg_df.writeStream \
        .format("parquet") \
        .option("path", "hdfs://hdfs-namenode:9870/northwind/aggregated/revenue") \
        .option("checkpointLocation", "/tmp/spark-checkpoint/agg") \
        .partitionBy("date") \
        .start()

    spark.streams.awaitAnyTermination()
    ```

12. **Cháº¡y Spark job**:

    ```bash
    docker exec -it spark-master spark-submit /app/spark_job.py
    ```

    > **Ká»³ vá»ng**: Dá»¯ liá»‡u tá»« Kafka Ä‘Æ°á»£c Ä‘á»c, xá»­ lÃ½, vÃ  lÆ°u vÃ o HDFS.

### Kiá»ƒm tra káº¿t quáº£

13. **Kiá»ƒm tra Kafka topics**:

    ```bash
    docker exec -it connect /kafka/bin/kafka-console-consumer.sh --bootstrap-server kafka:9092 --topic cdc.northwind.order_details --from-beginning
    ```

    **Ká»³ vá»ng**: Tháº¥y JSON messages tá»« Debezium.

14. **Kiá»ƒm tra dá»¯ liá»‡u trong HDFS**:

    ```bash
    docker exec -it hdfs-namenode hdfs dfs -ls /northwind/raw/order_details
    docker exec -it hdfs-namenode hdfs dfs -ls /northwind/aggregated/revenue
    ```

    **Ká»³ vá»ng**:
    - `/northwind/raw/order_details`: Dá»¯ liá»‡u thÃ´ tá»« `order_details`.
    - `/northwind/aggregated/revenue`: Tá»•ng doanh thu theo ngÃ y.

15. **Query HDFS báº±ng Spark SQL**:

    ```bash
    docker exec -it spark-master spark-shell
    ```

    ```scala
    val df = spark.read.parquet("hdfs://hdfs-namenode:9870/northwind/aggregated/revenue")
    df.show()
    ```

    **Ká»³ vá»ng**:
    ```plaintext
    +----------+-------------+------------+
    |date      |total_revenue|order_count|
    +----------+-------------+------------+
    |2025-09-30|180.00       |1           |
    +----------+-------------+------------+
    ```

### Clean há»‡ thá»‘ng

16. **Dá»«ng vÃ  xÃ³a container, volume, network**:

    ```bash
    docker-compose down -v
    ```

    > **LÆ°u Ã½**: Lá»‡nh nÃ y xÃ³a táº¥t cáº£ container, volume (MySQL, HDFS), vÃ  Kafka topics. Sao lÆ°u dá»¯ liá»‡u náº¿u cáº§n.

    ![Clean System](https://via.placeholder.com/600x200.png?text=Clean+há»‡+thá»‘ng)  
    *HÃ¬nh áº£nh minh há»a clean há»‡ thá»‘ng*

---

## ğŸ”¹ PhÃ¢n cÃ´ng cÃ´ng viá»‡c

| ThÃ nh viÃªn | Vai trÃ² | TrÃ¡ch nhiá»‡m chÃ­nh | Thá»i gian Æ°á»›c tÃ­nh |
|------------|---------|-------------------|-------------------|
| **ThÃ nh viÃªn 1 (Leader)** | Project Manager & DevOps | - Quáº£n lÃ½ tiáº¿n Ä‘á»™, há»p nhÃ³m<br>- Thiáº¿t láº­p Docker Compose<br>- CI/CD vá»›i GitHub Actions | 40% |
| **ThÃ nh viÃªn 2** | Database & CDC Specialist | - Táº¡o schema Northwind<br>- Cáº¥u hÃ¬nh Debezium connector<br>- Test CDC | 30% |
| **ThÃ nh viÃªn 3** | Streaming Processing Expert | - Viáº¿t Spark job (parse, transform, aggregation)<br>- Optimize streaming | 30% |
| **ThÃ nh viÃªn 4** | Data Storage & Analytics | - Cáº¥u hÃ¬nh HDFS<br>- Viáº¿t Spark SQL queries<br>- Setup Grafana (tÃ¹y chá»n) | 30% |
| **ThÃ nh viÃªn 5** | Tester & Documentation | - Test end-to-end<br>- Viáº¿t test cases<br>- Soáº¡n bÃ¡o cÃ¡o, slide, video | 20% |

---

## ğŸ”¹ Lá»‹ch trÃ¬nh dá»± Ã¡n (4 tuáº§n)

### Tuáº§n 1 (30/09 - 06/10): Setup & Thiáº¿t káº¿
- **Nhiá»‡m vá»¥**:
  - ThÃ nh viÃªn 1: Táº¡o repo, setup Docker Compose.
  - ThÃ nh viÃªn 2: Khá»Ÿi táº¡o Northwind schema, chÃ¨n dá»¯ liá»‡u máº«u.
  - ThÃ nh viÃªn 3: Setup Spark project (Python).
  - ThÃ nh viÃªn 4: Cáº¥u hÃ¬nh HDFS cluster (3 nodes).
  - ThÃ nh viÃªn 5: Táº¡o template bÃ¡o cÃ¡o.
- **Milestone**: MÃ´i trÆ°á»ng cháº¡y, schema sáºµn sÃ ng.

### Tuáº§n 2 (07/10 - 13/10): CDC & Streaming
- **Nhiá»‡m vá»¥**:
  - ThÃ nh viÃªn 2: Cáº¥u hÃ¬nh Debezium, test CDC.
  - ThÃ nh viÃªn 3: Viáº¿t Spark job Ä‘á»c Kafka, lÆ°u dá»¯ liá»‡u thÃ´.
  - ThÃ nh viÃªn 1: TÃ­ch há»£p Debezium-Kafka.
  - ThÃ nh viÃªn 4: Test lÆ°u HDFS.
  - ThÃ nh viÃªn 5: Test CDC, viáº¿t docs.
- **Milestone**: Dá»¯ liá»‡u tá»« MySQL â†’ Kafka â†’ HDFS (thÃ´).

### Tuáº§n 3 (14/10 - 20/10): Transform & Load
- **Nhiá»‡m vá»¥**:
  - ThÃ nh viÃªn 3: Implement aggregation (tá»•ng doanh thu, sá»‘ Ä‘Æ¡n hÃ ng).
  - ThÃ nh viÃªn 4: LÆ°u aggregation vÃ o HDFS, query Spark SQL.
  - ThÃ nh viÃªn 2: Test UPDATE/DELETE CDC.
  - ThÃ nh viÃªn 1: Setup error handling.
  - ThÃ nh viÃªn 5: Test pipeline, viáº¿t test cases.
- **Milestone**: Pipeline ETL hoÃ n chá»‰nh.

### Tuáº§n 4 (21/10 - 28/10): Dashboard & BÃ¡o cÃ¡o
- **Nhiá»‡m vá»¥**:
  - ThÃ nh viÃªn 4: Táº¡o dashboard Grafana (tÃ¹y chá»n).
  - ThÃ nh viÃªn 3: Optimize Spark job (watermarking).
  - ThÃ nh viÃªn 1: Deploy pipeline (CI/CD).
  - ThÃ nh viÃªn 5: HoÃ n thÃ nh bÃ¡o cÃ¡o, slide, video demo.
  - ToÃ n nhÃ³m: Review code, test cuá»‘i.
- **Milestone**: Demo hoÃ n chá»‰nh, ná»™p bÃ¡o cÃ¡o.

**Há»p**: Thá»© 7, 10h sÃ¡ng (Google Meet/Zalo).  
**Theo dÃµi**: GitHub Projects (Kanban board).

---

## ğŸ”¹ LÆ°u Ã½

- **Binlog MySQL**: Pháº£i báº­t binlog vÃ  dÃ¹ng format `ROW`.
- **Kafka topics**: Má»—i báº£ng (`customers`, `orders`, `order_details`) cÃ³ topic riÃªng (`cdc.northwind.<table>`).
- **HDFS partition**: Dá»¯ liá»‡u thÃ´ vÃ  aggregation partition theo ngÃ y Ä‘á»ƒ tá»‘i Æ°u query.
- **Timestamp**: Dá»¯ liá»‡u tá»« MySQL lÆ°u timestamp UTC, Spark sáº½ xá»­ lÃ½ náº¿u cáº§n mÃºi giá» +07:00.

---

## ğŸ”¹ Kháº¯c phá»¥c sá»± cá»‘

- **Debezium khÃ´ng gá»­i dá»¯ liá»‡u**:
  - Kiá»ƒm tra log:
    ```bash
    docker logs connect
    ```
  - Äáº£m báº£o binlog báº­t:
    ```bash
    docker exec -it mysql mysql -uroot -proot -e "SHOW VARIABLES LIKE 'log_bin';"
    ```

- **Spark job lá»—i**:
  - ThÃªm debug:
    ```python
    df.show()
    ```
  - Kiá»ƒm tra Kafka consumer:
    ```bash
    docker exec -it connect /kafka/bin/kafka-console-consumer.sh --bootstrap-server kafka:9092 --topic cdc.northwind.orders
    ```

- **HDFS permission lá»—i**:
  - Cháº¡y vá»›i user `hdfs`:
    ```bash
    docker exec -it hdfs-namenode hdfs dfs -chown -R hdfs:hdfs /northwind
    ```

---

## ğŸ”¹ CÃ´ng nghá»‡ sá»­ dá»¥ng

- **MySQL**: 8.0
- **Kafka**: Confluent 7.4.0
- **Debezium**: 2.3.0
- **Spark**: 3.4.0 (Structured Streaming)
- **HDFS**: Hadoop 3.2.1
- **Python**: 3.9 (pyspark, kafka-python)
- **Docker & Docker Compose**

---

## ğŸ“§ LiÃªn há»‡

Má»Ÿ issue trÃªn GitHub hoáº·c liÃªn há»‡ nhÃ³m qua [Zalo/Slack].